{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1. Image Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pylab import*\n",
    "im_rs_list = []\n",
    "for i in range(1, 11):\n",
    "    filename = str(i) + \".PNG\"\n",
    "    im = Image.open(filename)  # read the file\n",
    "    im_rs = im.resize((100, 100))   # resize as 100 by 100 pixels\n",
    "    im_rs_list.append(im_rs)\n",
    "print(im_rs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_rs_grey_m_list = []\n",
    "for i in im_rs_list:\n",
    "    im_rs_grey_m = array(i.convert('L'))  # Converting to greyscale arrays\n",
    "    im_rs_grey_m_list.append(im_rs_grey_m)\n",
    "print(im_rs_grey_m_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_rs_grey_m_flatten_list = []\n",
    "for i in im_rs_grey_m_list:\n",
    "    im_rs_grey_m_flatten = i.flatten()  # Flatten to a 1-D array\n",
    "    im_rs_grey_m_flatten_list.append(im_rs_grey_m_flatten)\n",
    "print(im_rs_grey_m_flatten_list)\n",
    "# Export to csv file\n",
    "import csv\n",
    "with open('assignment_2_solution_1_3.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
    "    writer.writerows(im_rs_grey_m_flatten_list)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Draw the histogram\n",
    "for i in im_rs_grey_m_flatten_list:\n",
    "    plt.hist(i,256)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_rs_grey_flatten_norm_list = []\n",
    "for i in im_rs_grey_m_flatten_list:\n",
    "    imhist, bins = histogram(i, 256, normed=True)\n",
    "    cdf = imhist.cumsum()\n",
    "    cdf = 255 * cdf / cdf[-1]\n",
    "    im2 = interp(i, bins[:-1], cdf)\n",
    "    im_rs_grey_flatten_norm_list.append(im2)\n",
    "for i in im_rs_grey_flatten_norm_list:\n",
    "    plt.hist(i, 256)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Part 2. Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import openpyxl\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load the file\n",
    "assignment2 = openpyxl.load_workbook('Assignment 2 text.xlsx')\n",
    "sheet = assignment2['Sheet1']\n",
    "review_column_data = sheet['B']\n",
    "\n",
    "review_list = []\n",
    "for i in review_column_data[1:len(review_column_data)]:\n",
    "    review_list.append(i.value)\n",
    "\n",
    "\n",
    "# Tokenization and lemmatization\n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t.isalpha()]\n",
    "\n",
    "\n",
    "# Remove Stopwords and punctuation directly in Countvectorizer\n",
    "my_stop_words = [t for t in stopwords.words('english')]\n",
    "self_defined_stop_words = ['wa', 'quot', 'ha', 'gt', 'also'] # These are extra stop words I defined after running the topics many times\n",
    "my_stop_words.extend(self_defined_stop_words)\n",
    "vectorizer = CountVectorizer(stop_words=my_stop_words, tokenizer=LemmaTokenizer(), encoding='utf-8', min_df=5,\n",
    "                             ngram_range=(1, 2))\n",
    "review_vect = vectorizer.fit_transform(review_list)\n",
    "print(doc_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "lda = LatentDirichletAllocation(n_components=6, random_state=0).fit(review_vect)\n",
    "doc_topic = lda.transform(review_vect)  # 1000 by 6\n",
    "term_document_matrix = doc_topic.transpose()  # 6 by 1000\n",
    "\n",
    "print(doc_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"======Topic distribution of the first 10 restaurant reviews:======\")\n",
    "# Create topic dict\n",
    "for i in range(0, 10):# Topic distribution for the first 10 restaurant reviews\n",
    "    print(doc_topic[i])\n",
    "for i in range(0, 10):\n",
    "    topic_weight = doc_topic[i]\n",
    "    print(\"The \" + str(i + 1) + \"th\" + \" restaurant review has top 2 popular topic indices:\" + \",\".join(\n",
    "        str(index) for index in topic_weight.argsort()[:-2 - 1:-1])) # Top 2 topics for the first 10 restaurant reviews\n",
    "\n",
    "print(\"=====Topic distribution of the first 10 movie reviews:======\")\n",
    "for i in range(500, 510):\n",
    "    print(doc_topic[i]) # Topic distribution for the first 10 movie reviews\n",
    "for i in range(500, 510):\n",
    "    topic_weight = doc_topic[i]\n",
    "    print(\"The \" + str(i + 1) + \"th\" + \" movie review has top 2 popular topic indices:\" + \",\".join(\n",
    "        str(index) for index in topic_weight.argsort()[:-2 - 1:-1])) # Top 2 topics for the first 10 movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4.\tFind the top-5 terms (terms with the top-5 highest weights) for each of the 6 topics. Based\n",
    "on those terms, describe what those topics are about.\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx))\n",
    "    print(\",\".join([terms[i] for i in topic.argsort()[:-5 - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
